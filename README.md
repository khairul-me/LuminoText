# Text Analysis Tool for Religious and Humanities Texts

![GitHub last commit](https://img.shields.io/github/last-commit/your-username/your-repo)
![Python Version](https://img.shields.io/badge/python-3.8%2B-blue)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Project Overview
This project implements an advanced text analysis tool designed specifically for religious and humanities texts. It combines natural language processing, machine learning, and visualization techniques to extract meaningful insights from complex philosophical and religious writings.

### Author
**Md Khairul Islam**  
Robotics and Computer Science Major  
Hobart and William Smith Colleges, Geneva, NY  
ðŸ“§ khairul.islam@hws.edu | ðŸ“ž (315) 878-9695

## Features
- ðŸ“‘ Text extraction from PDF documents
- ðŸ” Advanced topic modeling
- ðŸ’­ Sentiment analysis across themes
- â³ Timeline extraction and visualization
- ðŸ•¸ï¸ Concept network mapping
- ðŸ“Š Keyword frequency analysis
- ðŸŽ¨ Multiple visualization techniques

## Sample Analysis Results
Here are some key visualizations generated by the tool:

### 1. Concept Network
![Concept Network](path_to_concept_network.png)
*Network visualization showing interconnected concepts and their relationships*

### 2. Keyword Frequencies
![Keyword Frequencies](path_to_keyword_frequencies.png)
*Distribution of key terms and their occurrence frequencies*

### 3. Sentiment Analysis
![Sentiment Analysis](path_to_sentiment_analysis.png)
*Sentiment scores across different themes*

### 4. Topic Clusters
![Topic Clusters](path_to_topic_clusters.png)
*Word clouds representing different topic clusters*

## Technical Architecture

### Core Components
1. **Text Extraction Module**
   - PDF text extraction using PyMuPDF (fitz)
   - Structural preservation of document layout
   - Text cleaning and preprocessing

2. **Analysis Pipeline**
   - Topic modeling using LatentDirichletAllocation
   - Sentiment analysis using NLTK's VADER
   - Named Entity Recognition using spaCy
   - Text summarization using T5 Transformer

3. **Visualization Engine**
   - Network graphs using NetworkX
   - Statistical plots using Matplotlib
   - Word clouds using WordCloud
   - Timeline visualization

## Dependencies
```python
# Core Requirements
numpy>=1.21.0
pandas>=1.3.0
matplotlib>=3.4.0
seaborn>=0.11.0
scikit-learn>=0.24.0
networkx>=2.6.0
spacy>=3.1.0
PyMuPDF>=1.18.0
nltk>=3.6.0
wordcloud>=1.8.0
transformers>=4.9.0
torch>=1.9.0
community>=1.0.0b1

# Additional Requirements
python-dateutil>=2.8.0
typing>=3.7.4
logging>=0.5.1.2
warnings>=0.1.0
```

## Installation Guide

1. Clone the repository:
```bash
git clone https://github.com/your-username/your-repo.git
cd your-repo
```

2. Create and activate a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Download required NLTK data:
```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('vader_lexicon')
```

5. Download spaCy model:
```bash
python -m spacy download en_core_web_md
```

## Usage

1. Basic usage:
```python
from text_analyzer import EnhancedTextAnalyzer

# Initialize analyzer
analyzer = EnhancedTextAnalyzer(output_path="output/")

# Process document
analyzer.process_document("path/to/your/document.pdf")
```

2. Customizing analysis:
```python
# Adjust analysis parameters
analyzer.analyze_topics(num_topics=5)
analyzer.analyze_sentiment(custom_themes=['theme1', 'theme2'])
```

## Project Structure
```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ text_analyzer.py
â”‚   â”œâ”€â”€ visualization.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_analyzer.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_texts/
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ visualizations/
â”‚   â””â”€â”€ reports/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Methods and Techniques

### 1. Text Processing
- **PDF Extraction**: Uses PyMuPDF for accurate text extraction while preserving document structure
- **Preprocessing**: Custom cleaning pipeline including stop word removal, lemmatization, and special character handling
- **Tokenization**: Sentence and word tokenization using NLTK's proven algorithms

### 2. Analysis Methods
- **Topic Modeling**: Latent Dirichlet Allocation (LDA) for uncovering latent topics
- **Sentiment Analysis**: VADER sentiment analysis adapted for academic text
- **Network Analysis**: Graph-based analysis using NetworkX with community detection
- **Timeline Extraction**: Custom algorithm for identifying and organizing temporal events

### 3. Visualization Techniques
- **Network Graphs**: Force-directed layout for concept visualization
- **Statistical Plots**: Custom matplotlib implementations
- **Word Clouds**: Modified WordCloud generation for topic visualization
- **Timeline Plots**: Custom timeline visualization using matplotlib

## Contributing
Contributions are welcome! Please feel free to submit a Pull Request.

## License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments
- Hobart and William Smith Colleges for supporting this research
- [Kwame Anthony Appiah](https://en.wikipedia.org/wiki/Kwame_Anthony_Appiah) for the sample text used in development
- The digital humanities community for inspiration and methodologies

## Citations
If you use this tool in your research, please cite:
```
Islam, M.K. (2024). Text Analysis Tool for Religious and Humanities Texts. 
GitHub repository: https://github.com/your-username/your-repo
```

## Contact
For questions or collaboration opportunities, please contact:
- ðŸ“§ khairul.islam@hws.edu
- ðŸ“ž (315) 878-9695
